from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import ssl
import chromedriver_autoinstaller
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os
import requests
import json
import time

# bs4 ê¸°ë³¸ ì„¤ì •
headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'}

# ëª©ë¡ í˜ì´ì§€ URL
BASE_URL = 'https://www.albamon.com/alba-talk/experience'
TARGET_PAGE_URL = "https://www.albamon.com/alba-talk/experience?pageIndex={page}&searchKeyword=&sortType=CREATED_DATE"
TARGET_PAGE = 1330

# í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
post_data = []

'''
ChromeDriver Setting
'''
ssl._create_default_https_context = ssl._create_unverified_context  # SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™”

# ìë™ìœ¼ë¡œ ChromeDriver ì„¤ì¹˜
chromedriver_autoinstaller.install()

# Selenium ì‹¤í–‰ ì˜µì…˜ ì„¤ì •
chrome_options = Options()
chrome_options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# Chrome WebDriver ì‹¤í–‰
service = Service()
driver = webdriver.Chrome(service=service, options=chrome_options)


def get_soup(target_url, talkNo, headers):
    url = f'{target_url}/{talkNo}?sortType=CREATED_DATE'
    # ìš”ì²­ ë° BeautifulSoup ê°ì²´ ìƒì„±
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    return soup


def parse_comment_items(comment_list):
    # ë‚ ì§œì™€ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    comments_data = []
    for comment in comment_list:
        # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        comment_text_element = comment.select_one('.comment-list__text-override')
        comment_text = comment_text_element.get_text(strip=True) if comment_text_element else "N/A"

        # ë°ì´í„° ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
        comments_data.append(comment_text)
    return comments_data
    

'''
talkNoë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•¨
'''
def crawl_talk_no(pageIndex):
    print(f"ğŸš€ í˜ì´ì§€ {pageIndex} í¬ë¡¤ë§ ì‹œì‘...")
    driver.get(TARGET_PAGE_URL.format(page=pageIndex))
    time.sleep(3)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°

    buttons = WebDriverWait(driver, 10).until(
        EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".Button_button__S9rjD.Button_text__5x_Cn.Button_large___Kecx.tertiary")))
    
    # JavaScriptë¥¼ ì´ìš©í•´ í´ë¦­ (í´ë¦­ ì˜¤ë¥˜ ë°©ì§€)
    driver.execute_script("arguments[0].scrollIntoView();", buttons[0])
    driver.execute_script("arguments[0].click();", buttons[0])  

    time.sleep(2)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
    # í˜„ì¬ í˜ì´ì§€ URLì—ì„œ talkNo ì¶”ì¶œ
    current_url = driver.current_url
    talk_no = current_url.split("/")[-1].split("?")[0]  # talkNo ì¶”ì¶œ
    print(f"ğŸ”¹ ê²Œì‹œê¸€ URL: {current_url}")

    return talk_no


'''
ìµœê·¼ talkNo ë¶€í„° nê°œì˜ ê²Œì‹œë¬¼ í¬ë¡¤ë§
'''
def crawl_post_detail(START_TALK_NO, LAST_TALK_NO):
    for talk_no in range(START_TALK_NO, LAST_TALK_NO, -1):
        soup = get_soup(BASE_URL, talk_no, headers)
        # ë§ê¸€ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        parsed_comments = []

        # ì½˜í…ì¸  ì˜ì—­ (ê²Œì‹œê¸€ ì œëª©, ë‚´ìš©, ì‘ì„±ì¼ì)
        title_element = soup.select_one('.DetailTitle_detail__header--title__Bbp40')
        content_element = soup.select_one('.Detail_content__content__hJ5M7')
        date_element = soup.select_one('.CommonInfos_info__wrapper__aGcEl > div:nth-child(2)')
        view_count_element = soup.select_one('.experience__span--view')

        title = title_element.get_text(strip=True) if title_element else "N/A"
        contents = content_element.get_text(strip=True) if content_element else "N/A"
        content_date = date_element.get_text(strip=True) if date_element else "N/A"
        view_count = view_count_element.get_text(strip=True) if date_element else "N/A"
        # í•„ìˆ˜ê°’ì´ ì—†ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì €ì¥í•˜ì§€ ì•Šê³  ë„˜ì–´ê°€ê¸°
        if not title_element or not title_element.get_text(strip=True) or not contents or not content_date:
            continue


        # ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
        comment_list = soup.select('.CommentList_comment-contents__YVrtF > ul li')
        parsed_comments = parse_comment_items(comment_list)

        # ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        post_data.append({
            "talkNo": talk_no,
            "Title": title,
            "Contents": contents,
            "Date": content_date,
            "ViewCount": view_count,
            "Comments": parsed_comments
        })

        print(f"talkNo: {talk_no}")
        print(f"Title: {title}")
        print(f"Contents: {contents}")
        print(f"Date: {content_date}")
        print(f"comments : {parsed_comments}")
        print(f"ViewCount: {view_count}")
        print("-" * 80)

    return post_data
    

START_TALK_NO = int(crawl_talk_no(1))
LAST_TALK_NO = int(crawl_talk_no(TARGET_PAGE))

data = crawl_post_detail(START_TALK_NO, LAST_TALK_NO)

# ë¸Œë¼ìš°ì € ì¢…ë£Œ
driver.quit()

# ë°ì´í„°í”„ë ˆì„ ìƒì„±
df = pd.DataFrame(data)
# ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
output_dir = "crawling_result"
os.makedirs(output_dir, exist_ok=True)


# CSV íŒŒì¼ë¡œ ì €ì¥
csv_file =  os.path.join(output_dir, "crawling_combined_result.csv")
df.to_csv(csv_file, index=False, encoding='utf-8-sig')  # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì €ì¥
print(f"Data saved to {csv_file}")